{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs4, tkinter, requests With maybe matplotli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Creation, retrieve link from client\n",
    "# https://realpython.com/python-gui-tkinter/\n",
    "from bs4 import BeautifulSoup\n",
    "import tkinter as tk\n",
    "\n",
    "window = tk.Tk()\n",
    "greeting = tk.Label(\n",
    "    text=\"Hi! Wanna give me an EA post link?\",\n",
    "    fg=\"white\", \n",
    "    bg=\"black\",\n",
    "    width = 100,\n",
    "    height = 30\n",
    ")\n",
    "greeting.pack()\n",
    "\n",
    "label = tk.Label(\n",
    "    text = \"Insert Link Below\",\n",
    "    fg = \"white\",\n",
    "    bg = \"black\"\n",
    ")\n",
    "\n",
    "entry = tk.Entry(\n",
    "    fg=\"black\",\n",
    "    bg=\"white\",\n",
    "    width = 100\n",
    ")\n",
    "\n",
    "label.pack()\n",
    "entry.pack()\n",
    "link = entry.get()\n",
    "soup = BeautifulSoup(link)\n",
    "soup.find()\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying out soup\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllib2\n",
    "import requests\n",
    "import re\n",
    "def soupy_text(passedUrl) :\n",
    "    r = requests.get(passedUrl)\n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    comment_section = soup.find_all(\"div\", attrs={\"class\":'PostsPage-commentsSection'})\n",
    "\n",
    "    comment_sec = str(comment_section).split(\" \")\n",
    "    comms = []\n",
    "    current = \"\"\n",
    "    started = False\n",
    "    for comm in comment_sec :\n",
    "        if started and comm != \"</p>\":\n",
    "            current = current + comm\n",
    "        elif comm == \"<p>\" :\n",
    "            started = True\n",
    "            current = current + comm\n",
    "        if started and comm == \"</p>\" :\n",
    "            started = False\n",
    "            comms.append(current)\n",
    "            current = \"\"\n",
    "    return comms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soupy_text(\"https://forum.effectivealtruism.org/posts/FFY8zfkQq3qdELyvm/consider-working-more-hours-and-taking-more-stimulants-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kelli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/kelli/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mEven if all those things apply ... this post may not be for you! Last year I tried to replace sleep with caffeine, and it did not go well. Even if you think you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre happy and emotionally stable, you may discover that stimulants are anxiogenic for you, and you may be dumb enough (i.e., me) not to make that connection for a year.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m stopWords \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[1;32m     10\u001b[0m freqTable \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words :\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/kelli/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\n",
    "    \n",
    ")\n",
    "text = \"Even if all those things apply ... this post may not be for you! Last year I tried to replace sleep with caffeine, and it did not go well. Even if you think you're happy and emotionally stable, you may discover that stimulants are anxiogenic for you, and you may be dumb enough (i.e., me) not to make that connection for a year.\"\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(text)\n",
    "freqTable = dict()\n",
    "for word in words :\n",
    "    word = word.lower()\n",
    "    if word in stopWords :\n",
    "        freqTable[word] += 1\n",
    "    else :\n",
    "        freqTable[word] = 1\n",
    "print(freqTable)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display answer to client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
